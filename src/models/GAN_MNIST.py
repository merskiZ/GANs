import os, time, itertools, pickle
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

from GAN import GAN


class GANMNIST(GAN):
    def generator(self, x, params=None):
        """
        Generator with applying only flat tensor with fully connected network
        :param x: input tensor
        :param params: parameters for specific regulator or pooling layer
        :return:
        """
        # initializer
        w_init = tf.truncated_normal_initializer(mean=0, stddev=0.02)
        b_init = tf.constant_initializer(0.)

        # first hidden layer
        w0 = tf.get_variable('g_w0', [x.get_shape()[1], 256], initializer=w_init)
        b0 = tf.get_variable('g_b0', [256], initializer=b_init)
        h0 = tf.nn.relu(tf.matmul(x, w0) + b0)

        # second hidden layer
        w1 = tf.get_variable('g_w1', [h0.get_shape()[1], 512], initializer=w_init)
        b1 = tf.get_variable('g_b1', [512], initializer=b_init)
        h1 = tf.nn.relu(tf.matmul(h0, w1) + b1)

        # third hidden layer
        w2 = tf.get_variable('g_w2', [h1.get_shape()[1], 1024], initializer=w_init)
        b2 = tf.get_variable('g_b2', [1024], initializer=b_init)
        h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)

        # output hidden layer
        w3 = tf.get_variable('g_w3', [h2.get_shape()[1], 784], initializer=w_init)
        b3 = tf.get_variable('g_b3', [784], initializer=b_init)
        out = tf.nn.tanh(tf.matmul(h2, w3) + b3)

        return out

    def discriminator(self, x, drop_out=None, params=None):
        """
        Discriminator with applying only flat tensor with fully connected network
        :param x: input tensor
        :param params: parameters for specific regulator or pooling layer
        :return:
        """
        # drop_out = params['drop_out']

        # initializer
        w_init = tf.truncated_normal_initializer(mean=0, stddev=0.02)
        b_init = tf.constant_initializer(0.)

        # first hidden layer
        w0 = tf.get_variable('d_w0', [x.get_shape()[1], 1024], initializer=w_init)
        b0 = tf.get_variable('d_b0', [1024], initializer=b_init)
        h0 = tf.nn.relu(tf.matmul(x, w0) + b0)
        h0 = tf.nn.dropout(h0, drop_out)

        # second hidden layer
        w1 = tf.get_variable('d_w1', [h0.get_shape()[1], 512], initializer=w_init)
        b1 = tf.get_variable('d_b1', [512], initializer=b_init)
        h1 = tf.nn.relu(tf.matmul(h0, w1) + b1)
        h1 = tf.nn.dropout(h1, drop_out)

        # third hidden layer
        w2 = tf.get_variable('d_w2', [h1.get_shape()[1], 256], initializer=w_init)
        b2 = tf.get_variable('d_b2', [256], initializer=b_init)
        h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)
        h2 = tf.nn.dropout(h2, drop_out)

        # output layer
        w3 = tf.get_variable('d_w3', [h2.get_shape()[1], 1], initializer=w_init)
        b3 = tf.get_variable('d_b3', [1], initializer=b_init)
        output = tf.nn.sigmoid(tf.matmul(h2, w3) + b3)

        return output

    def loss_g(self, d_fake=None, params=None):
        """
        Calculate generator loss based on the output of discriminator against a fake image
        generated by the generator
        :param d_fake: a float that after a fake image goes through the discriminator network
        :param params:
        :return:
        """
        g_loss = tf.reduce_mean(-tf.log(d_fake) + float(params['eps']))
        return g_loss

    def loss_d(self, d_real=None, d_fake=None, params=None):
        """
        Calculate generator loss based on the output of discriminator against a real image
        from the input plus the counter part of generator discriminator loss
        :param d_fake: a float that after a fake image goes through the discriminator network
        :param d_real:
        :param d_fake:
        :param params:
        :return:
        """
        d_loss = tf.reduce_mean(-tf.log(d_real) - tf.log(1 - d_fake))
        return d_loss

    def __initialize_models__(self, params=None):
        """
        Initialize the generator and discriminator models.
        :param params:
        :return:
        """
        g_z, z = self.__initialize_generator__()
        d_real, d_fake, x, drop_out = self.__initialize_discriminator__(g_z, params)
        return d_real, d_fake, x, z, drop_out, g_z

    def __initialize_generator__(self, params=None):

        """
        setup discriminator
        :param params:
        :return:
        """
        with tf.variable_scope('g') as scope:
            z = tf.placeholder(tf.float32, shape=(None, 100))
            g_z = self.generator(z)
            return g_z, z

    def __initialize_discriminator__(self, g_z, params=None):
        """
        setup generator
        :param params:
        :return:
        """
        with tf.variable_scope('d') as scope:
            drop_out = tf.placeholder(dtype=tf.float32, name='drop_out')
            x = tf.placeholder(tf.float32, shape=(None, 784))
            d_real = self.discriminator(x, drop_out=drop_out,
                                        params=params)
            scope.reuse_variables()
            d_fake = self.discriminator(g_z, drop_out=drop_out, params=params)
            return d_real, d_fake, x, drop_out

    def show_result(self, num_epoch, sess, g_z,
                    drop_out, z,
                    show=False, save=False,
                    path='/tmp', is_fix=False):
        fixed_z_ = np.random.normal(0, 1, (25, 100))
        z_ = np.random.normal(0, 1, (25, 100))

        if is_fix:
            test_images = sess.run(g_z, {z: fixed_z_, drop_out: 0.0})
        else:
            test_images = sess.run(g_z, {z: z_, drop_out: 0.0})

        size_figure_grid = 5
        fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))
        for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):
            ax[i, j].get_xaxis().set_visible(False)
            ax[i, j].get_yaxis().set_visible(False)

        for k in range(5 * 5):
            i = k // 5
            j = k % 5
            ax[i, j].cla()
            ax[i, j].imshow(np.reshape(test_images[k], (28, 28)), cmap='gray')

        label = 'Epoch {0}'.format(num_epoch)
        fig.text(0.5, 0.04, label, ha='center')
        plt.savefig(os.path.join(path, 'result_epoch_{}.png'.format(num_epoch)))

        if show:
            plt.show()
        else:
            plt.close()

    def training_runner(self, training_data, validation_data,
                        testing_data=None, configs=None,
                        output_folder=None):
        """
        load data, model and start training
        :param training_data:
        :param validation_data:
        :param testing_data:
        :param configs:
        :param output_folder:
        :return:
        """
        params = configs['parameters']

        d_real, d_fake, x, z, drop_out, g_z = self.__initialize_models__(params=params)

        # get discriminator and generator loss
        d_loss = self.loss_d(d_real=d_real, d_fake=d_fake)
        g_loss = self.loss_g(d_fake=d_fake, params=params)

        # get trainable variables in the networks
        t_vars = tf.trainable_variables()
        d_vars = [var for var in t_vars if 'd_' in var.name]
        g_vars = [var for var in t_vars if 'g_' in var.name]

        # setup optimizer for networks
        d_optimizer = tf.train.AdamOptimizer(params['lr']).minimize(d_loss, var_list=d_vars)
        g_optimizer = tf.train.AdamOptimizer(params['lr']).minimize(g_loss, var_list=g_vars)

        # open session and initialize all variables
        sess = tf.InteractiveSession()
        tf.global_variables_initializer().run()

        # training loop
        np.random.seed(int(time.time()))
        start_time = time.time()
        train_epoch = params['train_epoch']
        batch_size = params['batch_size']

        for epoch in range(train_epoch):
            epoch_start_time = time.time()
            d_losses = []
            g_losses = []
            for i in range(training_data.shape[0] // batch_size):
                # update discriminator
                x_ = training_data[i * batch_size: (i + 1) * batch_size]
                z_ = np.random.normal(0, 1, (batch_size, 100))

                d_loss_, _ = sess.run([d_loss, d_optimizer], {x: x_, z: z_, drop_out: 0.3})
                d_losses.append(d_loss_)

                # update generator
                z_ = np.random.normal(0, 1, (batch_size, 100))
                g_loss_, _ = sess.run([g_loss, g_optimizer], {z: z_, drop_out: 0.3})
                g_losses.append(g_loss_)

            epoch_end_time = time.time()
            per_epoch_ptime = epoch_end_time - epoch_start_time
            print('[%d/%d] - ptime: %.2f loss_d: %.3f, loss_g: %.3f' %
                  ((epoch + 1), train_epoch,
                   per_epoch_ptime, np.mean(d_losses), np.mean(g_losses)))

            self.show_result(epoch, sess,
                             g_z, drop_out,
                             z, show=False,
                             is_fix=False,
                             path=output_folder)

        sess.close()